{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "782af24b-dc77-4ec0-9f3f-cf72a8b73d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "import wget\n",
    "import pathlib\n",
    "import pdfplumber\n",
    "import os\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22e05ec-a620-4e9f-af8d-e9d0f9bf7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://arxiv.org/pdf/1808.04295.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efefc67e-159e-4809-94d5-c82865e6d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaper(paper_url, filename=\"../data/random_paper.pdf\"):\n",
    "    \"\"\"\n",
    "    Downloads a paper from it's arxiv page and returns\n",
    "    the local path to that file.\n",
    "    TODO ; replace the filename with something better\n",
    "    \"\"\"\n",
    "    downloadedPaper = wget.download(paper_url, filename)    \n",
    "    downloadedPaperFilePath = pathlib.Path(downloadedPaper)\n",
    "\n",
    "    return downloadedPaperFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2277dd75-2ab7-41fa-ac58-0aff1916d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperFilePath = \"../data/random_paper.pdf\"\n",
    "paperContent = pdfplumber.open(paperFilePath).pages\n",
    "\n",
    "def displayPaperContent(paperContent, page_start=0, page_end=5):\n",
    "    for page in paperContent[page_start:page_end]:\n",
    "        print(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bef6d623-261d-4a27-9d75-594a5ad3c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPaperSummary(paperContent):\n",
    "    tldr_tag = \"\\n\\nTl;dr\"\n",
    "    openai.api_key = os.environ['openai_api_key']\n",
    "    engine_list = openai.Engine.list() # calling the engines available from the openai api \n",
    "    \n",
    "    for page in paperContent:    \n",
    "        text = page.extract_text() + tldr_tag\n",
    "        response = openai.Completion.create(engine=\"davinci\",prompt=text,temperature=0.3,\n",
    "            max_tokens=140,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=[\"\\n\"]\n",
    "        )\n",
    "        print(\"page start:  \" + response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bff5a7a1-9961-4b35-b072-bba06adb411c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page start:  : We show how to train deep neural networks by using Fourier analysis.\n",
      "page start:  : The frequency spectrum of the output of a DNN is determined by the frequency spectrum of the weights. The frequency spectrum of the output is proportional to the product of two factors: one is a decay term with respect to frequency; the other is the amplitude of the difference between the DNN output and the target function. Small initial weights lead to small amplitudes of high frequency components, thus leading the DNN output to a low complexity function with good generalization.\n",
      "page start:  : The loss function is the sum of the loss of each frequency, weighted by the amplitude and phase of the\n",
      "page start:  :\n",
      "page start:  \n",
      "page start:  :\n",
      "page start:  : The paper is about the effect of initializing a neural network. The authors find that the network’s performance is highly dependent on the initial weights. This is a problem because the weights are initialized randomly, and the network is not guaranteed to converge to a good solution.\n",
      "page start:  : The DNN is a universal function approximator, but it's not a universal function approximator.\n",
      "page start:  : Theoretical analysis shows that the generalization ability of DNNs depends on the frequency of the target function.\n",
      "page start:  : The Fourier transform is a mathematical tool for decomposing a signal into its frequency components. The Fourier transform of a function f (t) is a function F (k) that describes the amplitude of the frequency components of f (t) at each frequency k. The Fourier transform of a signal is a function F (k) that describes the amplitude of the frequency components of the signal at each frequency k. The Fourier transform is a linear operator, meaning that it is a function that takes a function as input and returns a function as output. The Fourier transform of a function f (t) is a function F (k) that describes the amplitude of the frequency components of\n",
      "page start:  :\n",
      "page start:  : Theorem 1 says that the L2 norm of the difference between the activations of two hidden units in a deep neural network is bounded by the L2 norm of the difference between the inputs to those units.\n",
      "page start:  \n",
      "page start:  : The paper shows that the authors have discovered a new way to make deep neural networks more stable. They do this by adding a new layer between the input and the output layer. The new layer is called the “residual” layer. The authors show that the residual layer helps stabilize the network and make it more robust to small perturbations.\n",
      "page start:  \n",
      "page start:  \n",
      "page start:  \n"
     ]
    }
   ],
   "source": [
    "paperContent = pdfplumber.open(paperFilePath).pages\n",
    "showPaperSummary(paperContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cacca63-37ac-4f08-a71f-d862ae985c97",
   "metadata": {},
   "source": [
    "## let's work on better PDF to text outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ed5e875-a314-4f61-948d-9fd4afce68e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding training and generalization in deep\n",
      "==============================\n",
      "learning by Fourier analysis\n",
      "==============================\n",
      "Zhi-QinJohnXu∗\n",
      "==============================\n",
      "8\n",
      "==============================\n",
      "NewYorkUniversityAbuDhabi\n",
      "==============================\n",
      "1\n",
      "==============================\n",
      "AbuDhabi129188,UnitedArabEmirates\n",
      "==============================\n",
      "0\n",
      "==============================\n",
      "zhiqinxu@nyu.edu\n",
      "==============================\n",
      "2\n",
      "==============================\n",
      " \n",
      "==============================\n",
      "v\n",
      "==============================\n",
      "o\n",
      "==============================\n",
      "N Abstract\n",
      "==============================\n",
      " \n",
      "==============================\n",
      "9\n",
      "==============================\n",
      "2 Background: It is still an open research area to theoretically understand why\n",
      "==============================\n",
      "  DeepNeuralNetworks(DNNs)—equippedwithmanymoreparametersthantrain-\n",
      "==============================\n",
      " \n",
      "==============================\n",
      "] ing data and trained by (stochastic) gradient-based methods—often achieve re-\n",
      "==============================\n",
      "G\n",
      "==============================\n",
      "markably low generalization error. Contribution: We study DNN training by\n",
      "==============================\n",
      "L Fourier analysis. Our theoretical frameworkexplains: i) DNN with (stochastic)\n",
      "==============================\n",
      ". gradient-based methods often endows low-frequency components of the target\n",
      "==============================\n",
      "s\n",
      "==============================\n",
      "c function with a higher priority during the training; ii) Small initialization leads\n",
      "==============================\n",
      "[ to good generalizationability of DNN while preservingthe DNN’s ability to ﬁt\n",
      "==============================\n",
      " \n",
      "==============================\n",
      "  any function. These results are further conﬁrmed by experiments of DNNs ﬁt-\n",
      "==============================\n",
      "4\n",
      "==============================\n",
      "tingthefollowingdatasets,thatis,naturalimages,one-dimensionalfunctionsand\n",
      "==============================\n",
      "v\n",
      "==============================\n",
      "MNISTdataset.\n",
      "==============================\n",
      "5\n",
      "==============================\n",
      "9\n",
      "==============================\n",
      "2\n",
      "==============================\n",
      "4 1 Introduction\n",
      "==============================\n",
      "0\n",
      "==============================\n",
      ".\n",
      "==============================\n",
      "8 Background Deep learning has achieved great success as in many ﬁelds (LeCunetal. (2015)).\n",
      "==============================\n",
      "0 Recent studies have focused on understandingwhy DNNs, trained by (stochastic) gradient-based\n",
      "==============================\n",
      "8\n",
      "==============================\n",
      "methods, can generalize well, that is, DNNs often ﬁt the test data well which are not used for\n",
      "==============================\n",
      "1\n",
      "==============================\n",
      "traininginpractice. Counter-intuitively,althoughDNNshavemanymoreparametersthantraining\n",
      "==============================\n",
      ":\n",
      "==============================\n",
      "v data,theycanrarelyoverﬁtthetrainingdatainpractice.\n",
      "==============================\n",
      "i\n",
      "==============================\n",
      "X Several studies have focused on the local property (sharpness/ﬂatness) of loss function at min-\n",
      "==============================\n",
      "r ima (Hochreiter&Schmidhuber(1995)) to explorethe DNN’s generalizationability. Keskaretal.\n",
      "==============================\n",
      "a (2016)empiricallydemonstratedthatwithsmallbatchineachtrainingstep,DNNsconsistentlycon-\n",
      "==============================\n",
      "verge to ﬂat minima, and lead to a better generalization. However, Dinhetal. (2017) arguedthat\n",
      "==============================\n",
      "most notionsof ﬂatness are problematic. To this end, Dinhetal. (2017) used deep networkswith\n",
      "==============================\n",
      "rectiﬁer linear units (ReLUs) to theoretically show that any minimum can be arbitrarily sharp or\n",
      "==============================\n",
      "ﬂatwithoutspecifyingparameterization. Withtheconstraintofsmallweightsin parameterization,\n",
      "==============================\n",
      "Wuetal.(2017)provedthatfortwo-layerReLUnetworks,low-complexitysolutionslieintheareas\n",
      "==============================\n",
      "withsmallHessian,thatis,ﬂatandlargebasinsofattractor(Wuetal.(2017)).Theythenconcluded\n",
      "==============================\n",
      "thatarandominitializationtendstoproducestartingparameterslocatedinthebasinofﬂatminima\n",
      "==============================\n",
      "withahighprobability,usinggradient-basedmethods.\n",
      "==============================\n",
      "Several studies rely on the concept of stochastic approximation or uniform stability\n",
      "==============================\n",
      "(Bousquet&Elisseeff (2002), Hardtetal. (2015)). To ensure the stability of a training algorithm,\n",
      "==============================\n",
      "Hardtetal.(2015)assumedlossfunctionwithgoodproperties,suchasLipschitzorsmoothcondi-\n",
      "==============================\n",
      "tions. However,thelossfunctionofaDNNisoftenverycomplicated(Zhangetal.(2016)).\n",
      "==============================\n",
      "∗ThisworkisdonewhileXuisavisitingmemberatCourantInstituteofMathematicalSciences,NewYork\n",
      "==============================\n",
      "University,NewYork,UnitedStates.\n",
      "==============================\n",
      "Preprint.Workinprogress.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "text = paperContent[0].extract_text()\n",
    "paragraphs = text.split('\\n')\n",
    "# print each paragraph\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph)\n",
    "    print('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97b429ee-130f-4efd-b71e-b5e7ebbaa930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Understanding training and generalization in deep\\nlearning by Fourier analysis\\nZhi-QinJohnXu∗\\n8\\nNewYorkUniversityAbuDhabi\\n1\\nAbuDhabi129188,UnitedArabEmirates\\n0\\nzhiqinxu@nyu.edu\\n2\\n \\nv\\no\\nN Abstract\\n \\n9\\n2 Background: It is still an open research area to theoretically understand why\\n  DeepNeuralNetworks(DNNs)—equippedwithmanymoreparametersthantrain-\\n \\n] ing data and trained by (stochastic) gradient-based methods—often achieve re-\\nG\\nmarkably low generalization error. Contribution: We study DNN training by\\nL Fourier analysis. Our theoretical frameworkexplains: i) DNN with (stochastic)\\n. gradient-based methods often endows low-frequency components of the target\\ns\\nc function with a higher priority during the training; ii) Small initialization leads\\n[ to good generalizationability of DNN while preservingthe DNN’s ability to ﬁt\\n \\n  any function. These results are further conﬁrmed by experiments of DNNs ﬁt-\\n4\\ntingthefollowingdatasets,thatis,naturalimages,one-dimensionalfunctionsand\\nv\\nMNISTdataset.\\n5\\n9\\n2\\n4 1 Introduction\\n0\\n.\\n8 Background Deep learning has achieved great success as in many ﬁelds (LeCunetal. (2015)).\\n0 Recent studies have focused on understandingwhy DNNs, trained by (stochastic) gradient-based\\n8\\nmethods, can generalize well, that is, DNNs often ﬁt the test data well which are not used for\\n1\\ntraininginpractice. Counter-intuitively,althoughDNNshavemanymoreparametersthantraining\\n:\\nv data,theycanrarelyoverﬁtthetrainingdatainpractice.\\ni\\nX Several studies have focused on the local property (sharpness/ﬂatness) of loss function at min-\\nr ima (Hochreiter&Schmidhuber(1995)) to explorethe DNN’s generalizationability. Keskaretal.\\na (2016)empiricallydemonstratedthatwithsmallbatchineachtrainingstep,DNNsconsistentlycon-\\nverge to ﬂat minima, and lead to a better generalization. However, Dinhetal. (2017) arguedthat\\nmost notionsof ﬂatness are problematic. To this end, Dinhetal. (2017) used deep networkswith\\nrectiﬁer linear units (ReLUs) to theoretically show that any minimum can be arbitrarily sharp or\\nﬂatwithoutspecifyingparameterization. Withtheconstraintofsmallweightsin parameterization,\\nWuetal.(2017)provedthatfortwo-layerReLUnetworks,low-complexitysolutionslieintheareas\\nwithsmallHessian,thatis,ﬂatandlargebasinsofattractor(Wuetal.(2017)).Theythenconcluded\\nthatarandominitializationtendstoproducestartingparameterslocatedinthebasinofﬂatminima\\nwithahighprobability,usinggradient-basedmethods.\\nSeveral studies rely on the concept of stochastic approximation or uniform stability\\n(Bousquet&Elisseeff (2002), Hardtetal. (2015)). To ensure the stability of a training algorithm,\\nHardtetal.(2015)assumedlossfunctionwithgoodproperties,suchasLipschitzorsmoothcondi-\\ntions. However,thelossfunctionofaDNNisoftenverycomplicated(Zhangetal.(2016)).\\n∗ThisworkisdonewhileXuisavisitingmemberatCourantInstituteofMathematicalSciences,NewYork\\nUniversity,NewYork,UnitedStates.\\nPreprint.Workinprogress.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a00e05-0384-4f9c-bcf0-6cf85b53c75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
